{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GatlaDeepthi/Deepthi_INFO5731_Spring2024/blob/main/Deepthi_Gatla_Exercise_03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdRwkJBn70nX"
      },
      "source": [
        "# **INFO5731 In-class Exercise 3**\n",
        "\n",
        "The purpose of this exercise is to explore various aspects of text analysis, including feature extraction, feature selection, and text similarity ranking.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission. , and no requests will be answered. Manage your time accordingly.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARqm7u6B70ne"
      },
      "source": [
        "## Question 1 (10 Points)\n",
        "Describe an interesting text classification or text mining task and explain what kind of features might be useful for you to build the machine learning model. List your features and explain why these features might be helpful. You need to list at least five different types of features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "VAZj4PHB70nf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "7b9205d0-9e11-4543-dcd2-6f5f22103183"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nPlease write you answer here:\\nCategorizing a news story based on its content is an interesting linguistic task.\\nThis can help news feeds and providers accurately categorize their content based on what topics they cover.\\nWhen presented with a piece of sports-related text, the model should be able to determine whether it belongs in the category of \"Sports.\"\\nEvery word in the article has a frequency value, which is denoted by \"word jumble\" feature.\\nIt is feasible to use the machine learning algorithm to link certain words with particular categories.\\nIdentified entities: This feature retrieves identifiable entities from a piece, such as persons, groups, and locations.\\nThe template may be able to identify which things belong into which categories.\\nTags relating to speech :This feature specifies each word\\'s morphological category, which can be noun, verb, or adjective.\\nIt is possible to train the model to correlate certain grammatical constructions with specific categories.\\nSentiment: This feature records the piece\\'s tone, which can be positive, negative, or neutral.\\nIt is feasible to train the model to link certain emotions with particular categories.\\nAccessibility: This feature measures the article\\'s accessibility by taking into account the median paragraph length and the complexity of the vocabulary.\\nIt is feasible to train the framework to correlate specific categories with different reading levels.\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Please write you answer here:\n",
        "Categorizing a news story based on its content is an interesting linguistic task.\n",
        "This can help news feeds and providers accurately categorize their content based on what topics they cover.\n",
        "When presented with a piece of sports-related text, the model should be able to determine whether it belongs in the category of \"Sports.\"\n",
        "Every word in the article has a frequency value, which is denoted by \"word jumble\" feature.\n",
        "It is feasible to use the machine learning algorithm to link certain words with particular categories.\n",
        "Identified entities: This feature retrieves identifiable entities from a piece, such as persons, groups, and locations.\n",
        "The template may be able to identify which things belong into which categories.\n",
        "Tags relating to speech :This feature specifies each word's morphological category, which can be noun, verb, or adjective.\n",
        "It is possible to train the model to correlate certain grammatical constructions with specific categories.\n",
        "Sentiment: This feature records the piece's tone, which can be positive, negative, or neutral.\n",
        "It is feasible to train the model to link certain emotions with particular categories.\n",
        "Accessibility: This feature measures the article's accessibility by taking into account the median paragraph length and the complexity of the vocabulary.\n",
        "It is feasible to train the framework to correlate specific categories with different reading levels.\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEUjBE6C70nf"
      },
      "source": [
        "## Question 2 (10 Points)\n",
        "Write python code to extract these features you discussed above. You can collect a few sample text data for the feature extraction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "EoQX5s4O70nf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e5b944a-1e67-4152-e01d-a278593d15b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: textstat in /usr/local/lib/python3.10/dist-packages (0.7.3)\n",
            "Requirement already satisfied: pyphen in /usr/local/lib/python3.10/dist-packages (from textstat) (0.14.0)\n",
            "{'davis': 1, 'eight': 1, 'points': 1, '25': 1, 'minutes': 1, 'missing': 1, '17': 1, 'games': 1, 'sprained': 1, 'left': 1, 'knee': 1, 'lakers': 1, 'got': 1, 'good': 1, 'nights': 1, 'bench': 1, 'malik': 1, 'monk': 1, 'carmelo': 1, 'anthony': 1, '.': 1}\n",
            "(S\n",
            "  (PERSON Davis/NNP)\n",
            "  had/VBD\n",
            "  eight/CD\n",
            "  points/NNS\n",
            "  in/IN\n",
            "  25/CD\n",
            "  minutes/NNS\n",
            "  after/IN\n",
            "  missing/VBG\n",
            "  17/CD\n",
            "  games/NNS\n",
            "  with/IN\n",
            "  a/DT\n",
            "  sprained/JJ\n",
            "  left/NN\n",
            "  knee/NN\n",
            "  and/CC\n",
            "  the/DT\n",
            "  (ORGANIZATION Lakers/NNP)\n",
            "  got/VBD\n",
            "  good/JJ\n",
            "  nights/NNS\n",
            "  off/IN\n",
            "  the/DT\n",
            "  bench/NN\n",
            "  from/IN\n",
            "  (PERSON Malik/NNP Monk/NNP)\n",
            "  and/CC\n",
            "  (PERSON Carmelo/NNP Anthony/NNP)\n",
            "  ./.)\n",
            "{'NNP': 6, 'VBD': 2, 'CD': 3, 'NNS': 4, 'IN': 5, 'VBG': 1, 'DT': 3, 'JJ': 2, 'NN': 3, 'CC': 2, '.': 1}\n",
            "{'polarity': 0.16666666666666666, 'subjectivity': 0.2166666666666667}\n",
            "{'readability': 65.39}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# You code here (Please add comments in the code):\n",
        "\n",
        "#Importing the libraries and modules\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import pos_tag, ne_chunk\n",
        "#Downloading the NLTK resources for NER\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "from textblob import TextBlob #Importing liberary for processing textual data\n",
        "!pip install textstat\n",
        "from textstat import flesch_reading_ease\n",
        "\n",
        "\n",
        "article_text = \"\"\"\n",
        "Davis had eight points in 25 minutes after missing 17 games with a sprained left knee and the Lakers got good nights off the bench from Malik Monk and Carmelo Anthony.\n",
        "\"\"\"\n",
        "#Downloading the NLTK resources---Tokenizer and  English stopwords\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "tokenized_text = word_tokenize(article_text.lower()) # Tokenizing the text into lowercase\n",
        "Words_filtered = [word for word in tokenized_text if word not in stopwords.words('english')] #removing the stopwords from the tokenized text\n",
        "\n",
        "#Collecting the frequency distribution of the filtered words and storing in a dictionary\n",
        "FrequencyDist_words = nltk.FreqDist(Words_filtered)\n",
        "Feature_worddist = dict(FrequencyDist_words)\n",
        "\n",
        "print(Feature_worddist) # printing the feature1\n",
        "\n",
        "#Named Entity Recognition Feature\n",
        "results = ne_chunk(pos_tag(word_tokenize(article_text))) # Doing NER on the tokenized text\n",
        "\n",
        "print(results)\n",
        "\n",
        "#POS Tag Feature\n",
        "\n",
        "pos_tags = pos_tag(word_tokenize(article_text)) # Provides the tokenized words its parts of speech\n",
        "# couning the each PAS tag\n",
        "pos_feature = {}\n",
        "for tag in pos_tags:\n",
        "    pos_feature[tag[1]] = pos_feature.get(tag[1], 0) + 1\n",
        "\n",
        "print(pos_feature)\n",
        "\n",
        "#Sentiment Analysis Feature\n",
        "\n",
        "sentiment_Text = TextBlob(article_text).sentiment  #Analysing the sentiment using TexyBlob\n",
        "sentimentAnalysis_feature = {'polarity': sentiment_Text.polarity, 'subjectivity': sentiment_Text.subjectivity}\n",
        "\n",
        "print(sentimentAnalysis_feature)\n",
        "\n",
        "#Readability Feature\n",
        "\n",
        "readability_Text = flesch_reading_ease(article_text) # calculating Flesch reading ease score\n",
        "readability_feature = {'readability': readability_Text}\n",
        "\n",
        "print(readability_feature)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oSK4soH70nf"
      },
      "source": [
        "## Question 3 (10 points):\n",
        "Use any of the feature selection methods mentioned in this paper \"Deng, X., Li, Y., Weng, J., & Zhang, J. (2019). Feature selection for text classification: A review. Multimedia Tools & Applications, 78(3).\"\n",
        "\n",
        "Select the most important features you extracted above, rank the features based on their importance in the descending order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "2CRuXfV570ng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93bb7405-d65c-43c4-910e-0b959c246d12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 3 Features:\n",
            "speeds 6.0\n",
            "games 3.0\n",
            "delay 3.0\n"
          ]
        }
      ],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "\n",
        "#Here I am using the Chi-square as the filter method\n",
        "\n",
        "#importing the required libraries\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "\n",
        "texts_article = [\n",
        "    \"Davis had eight points in 25 minutes after missing 17 games with a sprained left knee and the Lakers got good nights off the bench from Malik Monk and Carmelo Anthony.\",\n",
        "    \"The Biden administration announced Thursday it would delay part of its signature climate plan to slash planet-warming pollution from the power sector.\",\n",
        "    \"The original iPhone offered wifi, supported 2G EDGE connectivity and had internet download speeds below 500Kbps (compared to multi Mbps speeds today).\",\n",
        "    \"Scientists in Argentina discovered a new species of flying reptiles as long as a school bus known as 'The Dragon of Death'.\"\n",
        "]\n",
        "\n",
        "categories = [\"Sports\", \"Politics\", \"Technology\", \"Science\"]\n",
        "#combinig texts and categories\n",
        "alltexts = []\n",
        "for i in range(len(texts_article)):\n",
        "    alltexts.append(texts_article[i] + \" \" + categories[i])\n",
        "#performing vectorization---text documents into a matrix of token counts\n",
        "vectorizer = CountVectorizer(lowercase=True, stop_words='english')\n",
        "a = vectorizer.fit_transform(alltexts)\n",
        "b= np.array(categories)\n",
        "#feature selection\n",
        "selector = SelectKBest(chi2, k=3)\n",
        "selector.fit(a, b)\n",
        "#getting scores and feature name\n",
        "scores = selector.scores_ #retrieves the scores computed during the chi-squared test\n",
        "feature_names = np.array(vectorizer.get_feature_names_out())\n",
        "#sorting features\n",
        "sorted_indices = np.argsort(scores)[::-1]\n",
        "sorted_features = feature_names[sorted_indices]\n",
        "#printing top 3 features\n",
        "print(\"Top 3 Features:\")\n",
        "for i in range(3):\n",
        "    print(sorted_features[i], scores[sorted_indices[i]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nZGAOwl70ng"
      },
      "source": [
        "## Question 4 (10 points):\n",
        "Write python code to rank the text based on text similarity. Based on the text data you used for question 2, design a query to match the most relevant docments. Please use the BERT model to represent both your query and the text data, then calculate the cosine similarity between the query and each text in your data. Rank the similary with descending order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "b4HoWK-i70ng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c530ec82-2b99-4469-e74f-6c2eb61c3380"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.38.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "Most similar texts:\n",
            "The Los Angeles Lakers defeated the Brooklyn Nets 107-96 on Saturday. LeBron James led the Lakers with 32 points and 8 rebounds.\n"
          ]
        }
      ],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "!pip install transformers\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Example news article texts\n",
        "articletext = [\"The Los Angeles Lakers defeated the Brooklyn Nets 107-96 on Saturday. LeBron James led the Lakers with 32 points and 8 rebounds.\"]\n",
        "# Query\n",
        "query = \"LeBron James scored 32 points in Lakers' win over the Nets\"\n",
        "\n",
        "# Load BERT model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Tokenize the query and each article text\n",
        "querytokens = tokenizer.tokenize(query)\n",
        "articletokens = [tokenizer.tokenize(text) for text in articletext]\n",
        "\n",
        "# Convert the tokens to IDs\n",
        "queryids = tokenizer.convert_tokens_to_ids(querytokens)\n",
        "articleids = [tokenizer.convert_tokens_to_ids(tokens) for tokens in articletokens]\n",
        "\n",
        "# Pad the IDs to the maximum sequence length\n",
        "max_length = 64\n",
        "queryids = queryids[:max_length] + [0] * (max_length - len(queryids))\n",
        "articleids = [ids[:max_length] + [0] * (max_length - len(ids)) for ids in articleids]\n",
        "\n",
        "# Convert the IDs to PyTorch tensors\n",
        "queryids = torch.tensor(queryids).unsqueeze(0)\n",
        "articleids = torch.tensor(articleids)\n",
        "\n",
        "# Generate BERT embeddings for the query and each article text\n",
        "with torch.no_grad():\n",
        "    queryembedding = model(queryids)[0][:, 0, :].numpy()\n",
        "    articleembeddings = model(articleids)[0][:, 0, :].numpy()\n",
        "\n",
        "# Calculate the cosine similarity between the query and each article text\n",
        "similarities = cosine_similarity(queryembedding, articleembeddings).squeeze()\n",
        "\n",
        "# Sort the similarities in descending order and get the corresponding indices\n",
        "indices = np.argsort(similarities)[::-1]\n",
        "\n",
        "# Print the texts in descending order of similarity to the query\n",
        "print(\"Most similar texts:\")\n",
        "for i in indices:\n",
        "    print(articletext[i])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "VEs-OoDEhTW4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on this exercise**\n",
        "\n",
        "Please provide your thoughts and feedback on the exercises you completed in this assignment. Consider the following points in your response:\n",
        "\n",
        "Learning Experience: Describe your overall learning experience in working on extracting features from text data. What were the key concepts or techniques you found most beneficial in understanding the process?\n",
        "\n",
        "Challenges Encountered: Were there specific difficulties in completing this exercise?\n",
        "\n",
        "Relevance to Your Field of Study: How does this exercise relate to the field of NLP?\n",
        "\n",
        "**(Your submission will not be graded if this question is left unanswered)**\n",
        "\n"
      ],
      "metadata": {
        "id": "IUKC7suYhVl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Please write you answer here: Above I have learnt about the features like Word distribution analysis,NER ,POS tagging,sentiment and reading analysis.From these features extraction of text\n",
        "I have learnt how words are used and how to spot important names and places and how to figure out the role of each word in sentence and also performing the sentiment analysis and how easy\n",
        "the text is to read.And I have also learnt the Bert Model for Text similarity.Overall these features helped me to learn how to analyse the text and how to use them in differnt purposes.\n",
        "No specific difficulties but learnt some new things doing this exercise.\n",
        "Word distributions assist NLP systems with recognizing the significance and context of words within documents and this analysis also serves as the foundation for many NLP activities.\n",
        "The next feature NER has an important role in a different NLP applications like information extraction and entity linking. It lets systems to extract structured information from unstructured\n",
        "text, allowing them for more in-depth interpretation and analysis.Coming to next feature syntactic analysis, grammar verification, and text understanding tasks in NLP all require POS tagging.\n",
        "This feature POS tagging helps in the categorization of word meanings and the identification of grammatical structure in sentences, both of which are required for accurate language\n",
        "processing.BERT models capacity to capture contextual information, transfer learning skills, and cutting-edge performance have made it a standard in modern NLP research and applications.\n",
        "NLP systems may effectively handle and evaluate textual data by utilizing these properties, helping them to be used in a variety of fields.\n",
        "'''"
      ],
      "metadata": {
        "id": "CAq0DZWAhU9m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "outputId": "7c1802d7-3e09-4217-a8e3-c612ae6a6ce7"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nPlease write you answer here: Above I have learnt about the features like Word distribution analysis,NER ,POS tagging,sentiment and reading analysis.From these features extraction of text\\nI have learnt how words are used and how to spot important names and places and how to figure out the role of each word in sentence and also performing the sentiment analysis and how easy \\nthe text is to read.And I have also learnt the Bert Model for Text similarity.Overall these features helped me to learn how to analyse the text and how to use them in differnt purposes. \\nNo specific difficulties but learnt some new things doing this exercise.\\nWord distributions assist NLP systems with recognizing the significance and context of words within documents and this analysis also serves as the foundation for many NLP activities.\\nThe next feature NER has an important role in a different NLP applications like information extraction and entity linking. It lets systems to extract structured information from unstructured\\ntext, allowing them for more in-depth interpretation and analysis.Coming to next feature syntactic analysis, grammar verification, and text understanding tasks in NLP all require POS tagging.\\nThis feature POS tagging helps in the categorization of word meanings and the identification of grammatical structure in sentences, both of which are required for accurate language\\nprocessing.BERT models capacity to capture contextual information, transfer learning skills, and cutting-edge performance have made it a standard in modern NLP research and applications.\\nNLP systems may effectively handle and evaluate textual data by utilizing these properties, helping them to be used in a variety of fields.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}