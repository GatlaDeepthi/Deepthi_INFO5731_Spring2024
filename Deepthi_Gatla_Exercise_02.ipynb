{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GatlaDeepthi/Deepthi_INFO5731_Spring2024/blob/main/Deepthi_Gatla_Exercise_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DymRJbxDBCnf"
      },
      "source": [
        "# **INFO5731 In-class Exercise 2**\n",
        "\n",
        "The purpose of this exercise is to understand users' information needs, and then collect data from different sources for analysis by implementing web scraping using Python.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission. , and no requests will be answered. Manage your time accordingly.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBKvD6O_TY6e"
      },
      "source": [
        "## Question 1 (10 Points)\n",
        "Describe an interesting research question (or practical question or something innovative) you have in mind, what kind of data should be collected to answer the question(s)? Specify the amount of data needed for analysis. Provide detailed steps for collecting and saving the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cikVKDXdTbzE"
      },
      "source": [
        "QUESTION:\n",
        "\n",
        "How do job satisfaction and productivity among employees change in the post-pandemic period when flexible work arrangements (FWAs) are implemented?\n",
        "\n",
        "Following the COVID-19 pandemic, companies and employees are placing a great deal of emphasis on flexible work arrangements, or FWAs. In the post-pandemic period, it is critical to comprehend how these changes affect worker productivity and job satisfaction as remote work becomes more commonplace and hybrid models take shape. How does the adoption of flexible work arrangements (FWAs) affect workers' job happiness and productivity in the post-pandemic era? is the research topic that is asked.\n",
        "\n",
        "In order to provide a thorough response, a multimodal strategy to data collection is needed. First, in order to compare productivity and work satisfaction before and after the deployment of FWAs, quantitative data should be gathered. This can be done by giving out questionnaires to staff members and using pre-established measures to measure their subjective perceptions of productivity and work satisfaction. The questionnaires ought to cover a range of aspects related to the workplace, such as task accomplishment, job satisfaction, work-life equilibrium, and general job satisfaction.\n",
        "\n",
        "Depending on the size and variety of the workforce, a viable dataset for analysis would be produced with a sample size of at least 500 people from various departments and levels within the company. In order to detect any patterns or changes that might emerge over time, data should also be gathered longitudinally over a lengthy period of time—ideally, starting at least a year after the adoption of FWAs.\n",
        "\n",
        "Secondly, managers and staff should participate in focus groups or in-depth interviews to acquire qualitative data. These qualitative findings can help us comprehend the mechanisms by which FWAs affect job happiness and productivity better. Employing open-ended questions facilitates a more nuanced examination of the effects on individual and organizational dynamics by allowing employees to share their experiences, challenges, and perceptions on FWAs.\n",
        "\n",
        "In addition, gathering contextual information about the type of FWAs used by the company is essential. This contains details about the several FWAs that are available (such as shortened workweeks, flexible scheduling, and remote work), the level of managerial support and communication, and the availability of infrastructure that facilitates remote collaboration, such as technology.\n",
        "\n",
        "Steps to collect and save the data:\n",
        "\n",
        "Distribution of Surveys: Create and disseminate surveys via online survey tools like Google Forms and SurveyMonkey. Maintain secrecy and anonymity to promote truthful answers.\n",
        "Recruiting for Interviews and Focus Groups: Choose a representative group of managers and employees to take part in focus groups or interviews. Get informed consent, then arrange for convenient times for the sessions.\n",
        "Data recording and transcription: With participants' permission, record focus groups and interviews. Accurately transcribe the recordings while maintaining the depth of the qualitative information.\n",
        "Data Management and Storage: Keep survey data that is quantitative in a safe database that has access restrictions and suitable encryption. To preserve participant privacy, qualitative data—including transcripts and recordings—should be anonymised and kept in a secure location.\n",
        "Data analysis: Apply descriptive and inferential statistics to find patterns and relationships in the survey data using statistical tools like SPSS or R. To find important themes and insights, qualitative data should be thematically examined utilizing coding and thematic analysis methodologies.\n",
        "\n",
        "In summary, a thorough strategy to data collecting is needed to examine the effects of flexible work arrangements (FWAs) on worker productivity and job satisfaction. This approach should include quantitative surveys, qualitative interviews, and contextual data. Organizations can improve employee well-being and organizational performance in the post-pandemic period by optimizing their implementation tactics and gaining useful insights into the dynamics of flexible work arrangements through the systematic collection and analysis of data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9RqrlwdTfvl"
      },
      "source": [
        "## Question 2 (10 Points)\n",
        "Write Python code to collect a dataset of 1000 samples related to the question discussed in Question 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZOLkZOmACjc",
        "outputId": "8a56ef25-5ad4-4302-db20-6c23c3567cef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faker\n",
            "  Downloading Faker-23.2.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.4 in /usr/local/lib/python3.10/dist-packages (from faker) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.4->faker) (1.16.0)\n",
            "Installing collected packages: faker\n",
            "Successfully installed faker-23.2.0\n"
          ]
        }
      ],
      "source": [
        "pip install faker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iaBewZyACjd",
        "outputId": "f4d88f89-7f07-4ee9-d024-5246f697ac9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                              Employee_ID  Age      Gender  \\\n",
            "0    9591f34e-1a2d-4d3d-b818-a1f70d5fbb78   37        Male   \n",
            "1    90655399-972f-4b8a-bfbb-92dec131e398   37      Female   \n",
            "2    8de6a167-7b47-4437-b076-952b670bb990   51      Female   \n",
            "3    b860da03-52f0-4716-aa60-fe2fdbd1054d   42        Male   \n",
            "4    371edbc8-045f-4172-97ec-3089f5dd2662   28  Non-binary   \n",
            "..                                    ...  ...         ...   \n",
            "995  39dd237e-ba6d-480c-a363-ce9e28edc54a   57        Male   \n",
            "996  49cd3ae4-1cc3-4208-9c54-eb3245760efc   22        Male   \n",
            "997  c319e501-cdd9-4678-8c13-b0fae92ab817   24  Non-binary   \n",
            "998  9f94bdb1-9a5b-45d5-a565-d77b2048683b   48      Female   \n",
            "999  2db7906e-fabf-47e6-85ed-66a628c9de5a   40        Male   \n",
            "\n",
            "                          Job_Title             FWA_Type FWA_Frequency  \\\n",
            "0                     Tax inspector  Compressed Workweek    Frequently   \n",
            "1            Surveyor, hydrographic  Compressed Workweek        Rarely   \n",
            "2    Teaching laboratory technician          Remote Work    Frequently   \n",
            "3                     Social worker  Compressed Workweek        Rarely   \n",
            "4                    Phytotherapist          Remote Work  Occasionally   \n",
            "..                              ...                  ...           ...   \n",
            "995  Therapist, speech and language          Remote Work  Occasionally   \n",
            "996          Engineer, aeronautical          Remote Work        Rarely   \n",
            "997                  Quarry manager          Remote Work  Occasionally   \n",
            "998              Surveyor, minerals          Remote Work  Occasionally   \n",
            "999                Fashion designer       Flexible Hours    Frequently   \n",
            "\n",
            "     Job_Satisfaction  Productivity_Level  \n",
            "0                   4                   4  \n",
            "1                   2                   5  \n",
            "2                   5                   5  \n",
            "3                   1                   3  \n",
            "4                   4                   5  \n",
            "..                ...                 ...  \n",
            "995                 5                   2  \n",
            "996                 2                   4  \n",
            "997                 1                   1  \n",
            "998                 4                   3  \n",
            "999                 2                   5  \n",
            "\n",
            "[1000 rows x 8 columns]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "from faker import Faker\n",
        "\n",
        "fake = Faker()\n",
        "\n",
        "# Function to generate synthetic survey data\n",
        "def Survey_data_generation(num_samples):\n",
        "    DataSurvey = []\n",
        "    for _ in range(num_samples):\n",
        "        data = {\n",
        "            \"Employee_ID\": fake.uuid4(),\n",
        "            \"Age\": random.randint(22, 60),\n",
        "            \"Gender\": random.choice([\"Male\", \"Female\", \"Non-binary\"]),\n",
        "            \"Job_Title\": fake.job(),\n",
        "            \"FWA_Type\": random.choice([\"Remote Work\", \"Flexible Hours\", \"Compressed Workweek\"]),\n",
        "            \"FWA_Frequency\": random.choice([\"Rarely\", \"Occasionally\", \"Frequently\"]),\n",
        "            \"Job_Satisfaction\": random.randint(1, 5),  # Assume Likert scale from 1 to 5\n",
        "            \"Productivity_Level\": random.randint(1, 5),\n",
        "        }\n",
        "        DataSurvey.append(data)\n",
        "    return DataSurvey\n",
        "\n",
        "# Define the number of samples\n",
        "num_samples = 1000\n",
        "\n",
        "# Generate synthetic survey data\n",
        "DataSurvey = Survey_data_generation(num_samples)\n",
        "\n",
        "# Create a DataFrame\n",
        "DF = pd.DataFrame(DataSurvey)\n",
        "\n",
        "# Display the DataFrame\n",
        "print(DF)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03jb4GZsBkBS"
      },
      "source": [
        "## Question 3 (10 Points)\n",
        "Write Python code to collect 1000 articles from Google Scholar (https://scholar.google.com/), Microsoft Academic (https://academic.microsoft.com/home), or CiteSeerX (https://citeseerx.ist.psu.edu/index), or Semantic Scholar (https://www.semanticscholar.org/), or ACM Digital Libraries (https://dl.acm.org/) with the keyword \"XYZ\". The articles should be published in the last 10 years (2014-2024).\n",
        "\n",
        "The following information from the article needs to be collected:\n",
        "\n",
        "(1) Title of the article\n",
        "\n",
        "(2) Venue/journal/conference being published\n",
        "\n",
        "(3) Year\n",
        "\n",
        "(4) Authors\n",
        "\n",
        "(5) Abstract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "YaGLbSHHB8Ej",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a68f948-203e-4b9f-b58d-4792b8e65aea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                    paperId  \\\n",
            "0  4d161f4780efcf3f534ded2a6f64903a31546719   \n",
            "1  51f4b810996aae94bb559c27a15f2dd85061f432   \n",
            "2  e91afb2a4a156e73fbc7d6629f3a6798fe6c918c   \n",
            "\n",
            "                                                 url  \\\n",
            "0  https://www.semanticscholar.org/paper/4d161f47...   \n",
            "1  https://www.semanticscholar.org/paper/51f4b810...   \n",
            "2  https://www.semanticscholar.org/paper/e91afb2a...   \n",
            "\n",
            "                                               title  \\\n",
            "0                                       Temperature.   \n",
            "1  A generalized split-window algorithm for retri...   \n",
            "2  Colloquium : Theory of intertwined orders in h...   \n",
            "\n",
            "                                            abstract  \\\n",
            "0  This is the second in a series of articles on ...   \n",
            "1  Proposes a generalized split-window method for...   \n",
            "2  Understanding high temperature superconductors...   \n",
            "\n",
            "                                             journal  \\\n",
            "0  {'name': 'Nursing times', 'pages': '\n",
            "         ...   \n",
            "1  {'name': 'IEEE Trans. Geosci. Remote. Sens.', ...   \n",
            "2  {'name': 'Reviews of Modern Physics', 'pages':...   \n",
            "\n",
            "                                             authors  \n",
            "0  [{'authorId': '2283813730', 'name': 'Klamath B...  \n",
            "1  [{'authorId': '50385599', 'name': 'Z. Wan'}, {...  \n",
            "2  [{'authorId': '5685787', 'name': 'E. Fradkin'}...  \n"
          ]
        }
      ],
      "source": [
        "# write your answer here\n",
        "#Importing the required libraries and modules\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import time\n",
        "import requests\n",
        "import json\n",
        "\n",
        "#Creating the variable topic with the value as the keyword we need to search for in the semantic scholar articles.\n",
        "topic = 'Temperature'\n",
        "#Storing the query parameters and the dynamic query key that you need to search for in the URL variable.\n",
        "#Visit the URL\"https://api.semanticscholar.org/api-docs/graph\" to learn about query parameters, headers, and other requirements for accessing the api correctly.\n",
        "# In the meantime, data from an operating system or program can be accessed via an API (program Programming Interface).\n",
        "# As a result, APIs are dependent on the dataset's owner. The information may be made available for free or for a price.\n",
        "# Additionally, the owner has the ability to restrict how much data a user can access or how many queries they can make as an individual.\n",
        "url = \"https://api.semanticscholar.org/graph/v1/paper/search?query=\"+ topic + \"&offset=100&limit=3&fields=url,title,journal,authors,abstract\"\n",
        "\n",
        "#Since we are only using the GET API in this case, we must first create the empty payload and headers dictionary. Examine the supporting materials.\n",
        "payload={}\n",
        "headers = {}\n",
        "\n",
        "# Hitting the GET api using the request module, with \"GET\" method as the API method, URL with the dynamic URL created in the previous step, headers and payload.\n",
        "response = requests.request(\"GET\", url, headers=headers, data=payload)\n",
        "\n",
        "# Check if the response status code is OK (200)\n",
        "if response.status_code == 200:\n",
        "    # Loading the JSON response\n",
        "    data = response.json()\n",
        "    # Checking if the 'data' key exists in the response or not\n",
        "    if 'data' in data:\n",
        "        # Extracting the data and creating a DataFrame\n",
        "        df = pd.DataFrame(data['data'])\n",
        "        print(df.head())\n",
        "    else:\n",
        "        print(\"No 'data' key found in the response:\", data)\n",
        "else:\n",
        "    print(\"Error:\", response.text)\n",
        "\n",
        "\n",
        "#Just printing the response text from the above hit.\n",
        "##print(response.text)\n",
        "\n",
        "#As the data we received is text, we need to convert them into the json using the json.loads fucntion.\n",
        "#This conversion is possible as the text we received from GET method contains the formatted json data converted to string.\n",
        "##data = json.loads(response.text)['data']\n",
        "#Created the pandas dataframe object using the json data we generated in previous step.\n",
        "##df = pd.DataFrame(data)\n",
        "##df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJDe71iLB616"
      },
      "source": [
        "## Question 4A (10 Points)\n",
        "Develop Python code to collect data from social media platforms like Reddit, Instagram, Twitter (formerly known as X), Facebook, or any other. Use hashtags, keywords, usernames, or user IDs to gather the data.\n",
        "\n",
        "\n",
        "\n",
        "Ensure that the collected data has more than four columns.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55W9AMdXCSpV"
      },
      "source": [
        "## Question 4B (10 Points)\n",
        "If you encounter challenges with Question-4 web scraping using Python, employ any online tools such as ParseHub or Octoparse for data extraction. Introduce the selected tool, outline the steps for web scraping, and showcase the final output in formats like CSV or Excel.\n",
        "\n",
        "\n",
        "\n",
        "Upload a document (Word or PDF File) in any shared storage (preferably UNT OneDrive) and add the publicly accessible link in the below code cell.\n",
        "\n",
        "Please only choose one option for question 4. If you do both options, we will grade only the first one"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I57NXsauCec2"
      },
      "source": [
        "# write your answer here\n",
        "\n",
        "\n",
        "The tool that is selected to do the data extraction is Parse Hub.\n",
        "\n",
        "Even those without coding experience can easily extract data from websites with ParseHub, a potent web scraping application. Because of its powerful capabilities and user-friendly interface, it is a popular choice for both people and organizations.\n",
        "\n",
        "For this first Install the Parse Hub and then create an account and click on the \"New Project \" option available and\n",
        "provide the link from where you can extract the data .It will load and display its interface and it allows you to visualize\n",
        "the elements where you can extract the data from.\n",
        "\n",
        "You may quickly identify the precise data elements—such as text, images, links, and more—that you're interested in using\n",
        "ParseHub's point-and-click interface. To choose an element, just click on it. ParseHub will then automatically construct a\n",
        "selection command to extract the associated data.\n",
        "\n",
        "After you've decided which data items to scrape, you can fine-tune your project by specifying settings like pagination,\n",
        "AJAX handling, and data export choices, as well as by adding more commands.\n",
        "\n",
        "Once your project is configured, you can initiate the scraping process. ParseHub will then automatically crawl the website,\n",
        "extract the desired data items, and combine them into an Excel, JSON, or CSV file.\n",
        "\n",
        "You can download the data that has been extracted after the scraping process is finished and use it for reporting, analysis,\n",
        "or any other need you may have.\n",
        "\n",
        "All things considered, ParseHub makes web scraping easier by offering an intuitive user interface and robust capabilities\n",
        "that let users rapidly and effectively extract data from websites without requiring sophisticated coding knowledge or other\n",
        "technical skills.\n",
        "\n",
        "Considered URL: https://www.yellowpages.com/search?search_terms=car+insurance&geo_location_terms=New+York%2C+NY\n",
        "\n",
        "Link for the output :https://myunt-my.sharepoint.com/:x:/g/personal/deepthigatla_my_unt_edu/Efq1RCuvEb5KuwKZwo0y0qYBchxhdFvsNrodzBCrhQoVpA?e=AN6jJw\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZOhks1dXWEe"
      },
      "source": [
        "# Mandatory Question"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqmHVEwaWhbV"
      },
      "source": [
        "**Important: Reflective Feedback on Web Scraping and Data Collection**\n",
        "\n",
        "\n",
        "\n",
        "Please share your thoughts and feedback on the web scraping and data collection exercises you have completed in this assignment. Consider the following points in your response:\n",
        "\n",
        "\n",
        "\n",
        "Learning Experience: Describe your overall learning experience in working on web scraping tasks. What were the key concepts or techniques you found most beneficial in understanding the process of extracting data from various online sources?\n",
        "\n",
        "\n",
        "\n",
        "Challenges Encountered: Were there specific difficulties in collecting data from certain websites, and how did you overcome them? If you opted for the non-coding option, share your experience with the chosen tool.\n",
        "\n",
        "\n",
        "\n",
        "Relevance to Your Field of Study: How might the ability to gather and analyze data from online sources enhance your work or research?\n",
        "\n",
        "**(no grading of your submission if this question is left unanswered)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akAVJn9YBTQT"
      },
      "source": [
        "#ANSWER:\n",
        "\n",
        "While working on the topic Web scraping, I thought the entire process is diverse and enriching. The process of obtaining data\n",
        "from many internet sources required a comprehension of key topics including HTML parsing, CSS selectors, XPath, as well as\n",
        "HTTP requests. My ability to explore web pages using the Document Object Model (DOM) and comprehend various data structures\n",
        "improved significantly, which improved my performance on web scraping assignments.\n",
        "\n",
        "Advanced techniques such as headless browsers or AJAX handling were necessary to overcome the challenges posed by websites\n",
        "that loaded dynamic material via JavaScript. Furthermore, managing anti-scraping mechanisms put in place by websites,\n",
        "including rate limits or CAPTCHAs, required inventive ways to evade detection.\n",
        "\n",
        "The capacity to collect and evaluate data from internet sources is essential in future work. I can obtain insights from\n",
        "multiple sources, perform research more effectively, and keep up with current data trends thanks to it. Being able to use\n",
        "web data to influence decisions and draw meaningful conclusions improves the quality and scope of my work and study,\n",
        "whether it be in academia, business, or any other field."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MAlm_KpUACjg"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "jJDe71iLB616",
        "55W9AMdXCSpV"
      ],
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}